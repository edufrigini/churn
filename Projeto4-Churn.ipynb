{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Formação Cientista de Dados\n",
    "Projeto com Feedback 4\n",
    "    </font>\n",
    "\n",
    "## Eduardo Frigini de Jesus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prevendo Customer Churn em Operadoras de Telecom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os módulos\n",
    "import pandas as pd                 \n",
    "import matplotlib.pyplot as plt    \n",
    "import numpy as np         \n",
    "import os\n",
    "%matplotlib inline             \n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# import xlrd\n",
    "# import xlsxwriter\n",
    "\n",
    "# from IPython.display import display\n",
    "# from ipywidgets import widgets\n",
    "# from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset\n",
    "df_treino = pd.read_csv(\"projeto4_telecom_treino.csv\")\n",
    "df_teste = pd.read_csv(\"projeto4_telecom_teste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = df_treino.set_index('Key')\n",
    "df_teste = df_teste.set_index('Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_treino) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o formato dos dados\n",
    "df_treino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando as primeiras linhas do dataset\n",
    "df_treino.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando se existem valores nulos\n",
    "df_treino.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando as últimas linhas do dataset\n",
    "df_treino.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustando os datasets, substituindo yes e no por 1 e 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as classes. Substituindo o yes por 1 e no por 0\n",
    "change_map = {'yes' : 1, 'no' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o mapeamento ao dataset de treino\n",
    "df_treino['churn'] = df_treino['churn'].map(change_map)\n",
    "df_treino['international_plan'] = df_treino['international_plan'].map(change_map)\n",
    "df_treino['voice_mail_plan'] = df_treino['voice_mail_plan'].map(change_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o mapeamento ao dataset de teste\n",
    "df_teste['churn'] = df_teste['churn'].map(change_map)\n",
    "df_teste['international_plan'] = df_teste['international_plan'].map(change_map)\n",
    "df_teste['voice_mail_plan'] = df_teste['voice_mail_plan'].map(change_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificando e Balanceando os dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando como os dados estão distribuídos, no treino\n",
    "num_true = len(df_treino.loc[df_treino['churn'] == True])\n",
    "num_false = len(df_treino.loc[df_treino['churn'] == False])\n",
    "print(\"Número de Casos Verdadeiros: {0} ({1:2.2f}%)\".format(num_true, (num_true/ (num_true + num_false)) * 100))\n",
    "print(\"Número de Casos Falsos     : {0} ({1:2.2f}%)\".format(num_false, (num_false/ (num_true + num_false)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceando os dados de treino\n",
    "df_treino_true = (df_treino.loc[df_treino['churn']==True])\n",
    "df_treino_false = (df_treino.loc[df_treino['churn']==False]).sample(483)\n",
    "df_treino_balance = pd.concat([df_treino_true, df_treino_false])\n",
    "df_treino_balance.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando como os dados estão distribuídos, no treino\n",
    "num_true = len(df_treino_balance.loc[df_treino_balance['churn'] == True])\n",
    "num_false = len(df_treino_balance.loc[df_treino_balance['churn'] == False])\n",
    "print(\"Número de Casos Verdadeiros: {0} ({1:2.2f}%)\".format(num_true, (num_true/ (num_true + num_false)) * 100))\n",
    "print(\"Número de Casos Falsos     : {0} ({1:2.2f}%)\".format(num_false, (num_false/ (num_true + num_false)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando como os dados estão distribuídos, no teste\n",
    "num_true = len(df_teste.loc[df_teste['churn'] == True])\n",
    "num_false = len(df_teste.loc[df_teste['churn'] == False])\n",
    "print(\"Número de Casos Verdadeiros: {0} ({1:2.2f}%)\".format(num_true, (num_true/ (num_true + num_false)) * 100))\n",
    "print(\"Número de Casos Falsos     : {0} ({1:2.2f}%)\".format(num_false, (num_false/ (num_true + num_false)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceando os dados de teste\n",
    "df_teste_true = (df_teste.loc[df_teste['churn']==True])\n",
    "df_teste_false = (df_teste.loc[df_teste['churn']==False]).sample(224)\n",
    "df_teste_balance = pd.concat([df_teste_true, df_teste_false])\n",
    "df_teste_balance.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando como os dados estão distribuídos, no teste\n",
    "num_true = len(df_teste_balance.loc[df_teste_balance['churn'] == True])\n",
    "num_false = len(df_teste_balance.loc[df_teste_balance['churn'] == False])\n",
    "print(\"Número de Casos Verdadeiros: {0} ({1:2.2f}%)\".format(num_true, (num_true/ (num_true + num_false)) * 100))\n",
    "print(\"Número de Casos Falsos     : {0} ({1:2.2f}%)\".format(num_false, (num_false/ (num_true + num_false)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando a correlação entre as variáveis\n",
    "# Correlação não implica causalidade\n",
    "def plot_corr(df, size=10):\n",
    "    corr = df.corr()    \n",
    "    fig, ax = plt.subplots(figsize = (size, size))\n",
    "    ax.matshow(corr)  \n",
    "    plt.xticks(range(len(corr.columns)), corr.columns) \n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o gráfico\n",
    "plot_corr(df_treino_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando a correlação em tabela\n",
    "# Coeficiente de correlação: \n",
    "# +1  = forte correlação positiva\n",
    "# 0   = não há correlação\n",
    "# -1  = forte correlação negativa\n",
    "df_treino_balance.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decidi juntar os dois dataframes e separar novamente\n",
    "df = pd.concat([df_treino_balance, df_teste_balance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando como os dados estão distribuídos\n",
    "num_true = len(df.loc[df['churn'] == True])\n",
    "num_false = len(df.loc[df['churn'] == False])\n",
    "print(\"Número de Casos Verdadeiros: {0} ({1:2.2f}%)\".format(num_true, (num_true/ (num_true + num_false)) * 100))\n",
    "print(\"Número de Casos Falsos     : {0} ({1:2.2f}%)\".format(num_false, (num_false/ (num_true + num_false)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting\n",
    "\n",
    "70% para dados de treino e 30% para dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleção de variáveis preditoras (Feature Selection)\n",
    "atributos = ['international_plan', 'voice_mail_plan', 'number_vmail_messages', 'total_day_minutes', 'total_day_charge', 'total_eve_minutes', 'total_eve_charge', 'number_customer_service_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variável a ser prevista\n",
    "atrib_prev = ['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando objetos\n",
    "X = df[atributos].values\n",
    "Y = df[atrib_prev].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a taxa de split\n",
    "split_test_size = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dados de treino e de teste\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = split_test_size, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo os resultados\n",
    "print(\"{0:0.2f}% nos dados de treino\".format((len(X_treino)/len(df.index)) * 100))\n",
    "print(\"{0:0.2f}% nos dados de teste\".format((len(X_teste)/len(df.index)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando o Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original True : {0} ({1:0.2f}%)\".format(len(df.loc[df['churn'] == 1]), \n",
    "                                               (len(df.loc[df['churn'] ==1])/len(df.index) * 100)))\n",
    "\n",
    "print(\"Original False : {0} ({1:0.2f}%)\".format(len(df.loc[df['churn'] == 0]), \n",
    "                                               (len(df.loc[df['churn'] == 0])/len(df.index) * 100)))\n",
    "print(\"\")\n",
    "print(\"Training True : {0} ({1:0.2f}%)\".format(len(Y_treino[Y_treino[:] == 1]), \n",
    "                                               (len(Y_treino[Y_treino[:] == 1])/len(Y_treino) * 100)))\n",
    "\n",
    "print(\"Training False : {0} ({1:0.2f}%)\".format(len(Y_treino[Y_treino[:] == 0]), \n",
    "                                               (len(Y_treino[Y_treino[:] == 0])/len(Y_treino) * 100)))\n",
    "print(\"\")\n",
    "print(\"Test True : {0} ({1:0.2f}%)\".format(len(Y_teste[Y_teste[:] == 1]), \n",
    "                                               (len(Y_teste[Y_teste[:] == 1])/len(Y_teste) * 100)))\n",
    "\n",
    "print(\"Test False : {0} ({1:0.2f}%)\".format(len(Y_teste[Y_teste[:] == 0]), \n",
    "                                               (len(Y_teste[Y_teste[:] == 0])/len(Y_teste) * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valores Missing Ocultos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando se existem valores nulos\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo e treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando um classificador Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo preditivo\n",
    "modelo_v1 = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "modelo_v1.fit(X_treino, Y_treino.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando a exatidão no modelo nos dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predict_train = modelo_v1.predict(X_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exatidão (Accuracy): {0:.4f}\".format(metrics.accuracy_score(Y_treino, nb_predict_train)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando a exatidão no modelo nos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predict_test = modelo_v1.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exatidão (Accuracy): {0:.4f}\".format(metrics.accuracy_score(Y_teste, nb_predict_test)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma Confusion Matrix\n",
    "print(\"Confusion Matrix\")\n",
    "\n",
    "print(\"{0}\".format(metrics.confusion_matrix(Y_teste, nb_predict_test, labels = [1, 0])))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(Y_teste, nb_predict_test, labels = [1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizando o modelo com RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_v2 = RandomForestClassifier(random_state = 42)\n",
    "modelo_v2.fit(X_treino, Y_treino.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando os dados de treino\n",
    "rf_predict_train = modelo_v2.predict(X_treino)\n",
    "print(\"Exatidão (Accuracy): {0:.4f}\".format(metrics.accuracy_score(Y_treino, rf_predict_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando nos dados de teste\n",
    "rf_predict_test = modelo_v2.predict(X_teste)\n",
    "print(\"Exatidão (Accuracy): {0:.4f}\".format(metrics.accuracy_score(Y_teste, rf_predict_test)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "\n",
    "print(\"{0}\".format(metrics.confusion_matrix(Y_teste, rf_predict_test, labels = [1, 0])))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(Y_teste, rf_predict_test, labels = [1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terceira versão do modelo usando Regressão Logística\n",
    "modelo_v3 = LogisticRegression(C = 0.7, random_state = 42)\n",
    "modelo_v3.fit(X_treino, Y_treino.ravel())\n",
    "lr_predict_test = modelo_v3.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exatidão (Accuracy): {0:.4f}\".format(metrics.accuracy_score(Y_teste, lr_predict_test)))\n",
    "print()\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(Y_teste, lr_predict_test, labels = [1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resumindo\n",
    "## Exatidão nos dados de teste\n",
    "\n",
    "# Modelo usando algoritmo Naive Bayes         = 0.8306\n",
    "# Modelo usando algoritmo Random Forest       = 0.8612\n",
    "# Modelo usando algoritmo Regressão Logística = 0.7906"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo Previsões Com o Modelo Treinado\n",
    "## Random Forest foi o melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo para usar mais tarde\n",
    "filename = 'modelo_treinado_v2.sav'\n",
    "pickle.dump(modelo_v2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo e fazendo previsão com novos conjuntos de dados \n",
    "# (X_teste, Y_teste devem ser novos conjuntos de dados preparados com o procedimento de limpeza e transformação adequados)\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "resultado1 = loaded_model.predict(X_teste[10].reshape(1, -1))\n",
    "resultado2 = loaded_model.predict(X_teste[40].reshape(1, -1))\n",
    "print(resultado1)\n",
    "print(resultado2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
